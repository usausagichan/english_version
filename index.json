[{"categories":["Project"],"contents":"專案簡介 對品牌來說，藉由現有客戶引進新客戶有時是比大量投放廣告更有效率的方法。比如對手機遊戲來說，朋友是不是也有在玩很多時候會影響你繼續玩下去的意願。想像一下如果全班都在玩某款手遊，你是不是也會忍不住載來玩看看呢？因此很多手機遊戲經常會推出邀請朋友送道具的活動，為的就是要吸引你和你的朋友同時玩這款遊戲，增加新客-你的朋友的同時，也讓你們兩個在遊戲的留存率因為對方也在玩的關係跟著提高。 但是如果你是企劃，在推薦計畫推出前是不是得先用少部分的玩家先測試一下方案的效果呢？這時就需要用A/B Test來驗證方案效果。 這個專案便是用(https://datamasked.com/)網站中的資料練習A/B Testing的實作，確定某公司實施的推薦計劃是否有效（或有助於公司賺取更多的收入）。 我認為推薦計劃的成功應該定義為： （1）通過推薦計劃找到有價值的用戶（即推薦用戶的消費高於普通用戶）， （2）促進銷售或增加整體消費。\n為了確定推薦計劃是否有效，我們需要回答以下問題： （1）推薦計劃的用戶是否比普通用戶消費更多？ （2）推薦計劃是否能增加收入？\n完整程式碼可見於 本連結 ####查看資料 先大概看過有哪些column、並檢查資料的格式是不是恰當的\n####Task(1)推薦計劃的用戶是否比普通用戶消費更多？ 為了回答這個問題，我將資料拆分為: 控制組:非來自推薦計畫的使用者(referrral=0) 實驗組:來自推薦計畫的使用者(referrral=1) 並進行A/B Test檢驗兩者是否有不同 在檢驗進行前我先大致看過資料的分佈和視覺化成果 比較上圖test_data和control_data可發現兩者起始日期不同(2015-10-31\u0026amp;2015-10-03)，推測是因為推薦計畫的開始日期是10/31的緣故，因此若要比較控制組和實驗組應該都取10/31後的資料。\n","permalink":"https://usausagichan.github.io/english_version/blog/abtest_english/","tags":["Python","A/B Testing"],"title":"A/B Testing - User referral Program"},{"categories":["Project"],"contents":"平時對化妝品完全沒有概念，但想幫女朋友或朋友挑唇膏當聖誕禮物？別擔心，不用辛苦的看一堆網路文章，觀察PTT網友的用字遣詞自然會告訴你該怎麼挑唇膏\n專案簡介 利用python 抓取PTT MakeUP版2022年9/6到12/28所有有關唇膏的文章，進行關鍵字篩選及統計後以K-mean分群法將顧客分成五大群，並對他們各自喜好的唇膏顏色做統計分析。\n在這個專案中，我定義留言者對關鍵字使用的多寡為他對關鍵字的需求或偏好。比如一位留言者有護唇膏的需求，我便假設他會在留言中多次提及唇膏。而K-means分群正可以將不同維度中量值相似的向量區分出來，在專案中對應到的就是對一些關鍵字使用程度類似的留言者，我們可以說他們的偏好相近。因此，藉由K-means分群便可以近一步了解市場上偏好各異的消費者族群。 專案中所使用到的code可見於 本連結\n1.爬取PTT貼文 為了了解秋冬流行的唇膏，我利用Beautiful Soup 套件抓取9/6到12/28所有文章的標題、文章、留言和作者及留言者，並篩選出標題或文章內包含「唇」的貼文，確保資料探討的內容和唇妝有關。我們可以利用jieba切詞切分出文章出現的字詞，切詞後的貼文可見於下圖 小技巧：切分關鍵詞後可以事先刪除常見字尾或贅詞、PTT符號節省後面處理資料的時間。此外，事先將dict.txt.big.txt載入jieba切詞的字典裡可以更好的切分繁體字詞。\n2.篩選關鍵字 為了使jieba套件更好的切出想要的關鍵字，需要將與唇膏有關的關鍵字事先加入jieba的字典裡。由於這個project想探討的是有關顧客對唇膏的偏好，因此我先在網路上搜尋有關挑選唇膏的大原則 1 和化妝方式、技巧的文章 2 3，看出大家選購唇膏原則上依循兩大要點:\n妝感，大致可分為水潤（亮面）、霧面及介於兩者之間的水霧三種，而相關產品又可分為唇膏、唇釉（唇露/唇萃）、唇蜜、口紅四種\n唇膏顏色，除了自身喜好和流行，女生也會依據自己的膚色選擇適合自己的顏色，比如粉色較適合皮膚白皙的女生而不是和小麥膚色的女生。另外還會考慮唇膏本身的顯色度、飽和度和能不能顯白等等\n化妝技巧的部分秋冬因為有嘴唇乾燥的問題會先上一層護唇膏，為了避免原來唇色影響唇膏顯色或沾黏的問題會先抹上蜜粉打底和遮瑕。此外，最近韓系的暈染唇妝也很流行。唇膏的薄擦或後擦、疊色則是因人喜好而異。其他還有關於唇膏在日常使用的掉色程度、保濕度、滋潤度等等的考量\n對妝容感覺的偏好我分為可愛、仙氣、性感、優雅知性等等的風格\n參考這些文章後，我將這些關鍵字和專有名詞加入 excel檔，並載入jieba的字典裡，再利用jieba切詞將文章標題和內文切成字詞後進行統計，並存成excel檔。 我們可以看到使用頻率最頻繁的十個字如圖所示：\n表中有一些和美妝無關或無法從字詞得到資訊的字詞（例如：都、喜歡、好看、擦、美\u0026hellip;）， 這些詞會妨礙我們的分群觀察，所以我將這些字去除。 去除無關的關鍵字後，根據字詞出現的頻率畫出文字雲：\n可以發現聲量最多的關鍵字有：唇膏、唇彩、霧面、秋冬、唇釉、護唇膏、唇紋等等，回去看PTT幾篇文可以發現秋天最流行的唇妝是霧面，且大家普遍有乾唇問題所以會去找護唇膏。\n另外，我假設出現頻率5次以上的字詞都是jieba切得比較精準的字，所以我判斷該字與唇膏有關後便會直接保留這些關鍵字。至於其他關鍵字我則用程式篩選\n至於我篩選這些出現頻率4次以下關鍵字的方式如下：\n觀察聲量較高的關鍵字，歸納出與這些關鍵字意思類似的字可能包含哪個字，例如化妝品產品或PTT網友表達霧面的字可能有霧光、柔霧、霧唇等等。而我利用程式篩選出所有包含霧的關鍵字，再看關鍵字的字面意思決定要不要將關鍵字保留。例如「噴霧」雖然也包含誤字，但顯然和霧面無關，因此我會刪除。而若該字與霧面有相似意思，例如「霧唇」，我會將該字保留\n由於有關顏色的關鍵字很雜又很廣，例如光是粉色可能就有「粉紅」、「粉」、「偏粉」、「粉嫩」等等的表達方式，因此我也用程式篩選出可能表達顏色的字，比如想找出所有表達粉紅的關鍵字就先篩選出所有包含「粉」的字再進行人為判斷。\n在找大家討論的顏色時，我除了會上網查秋冬流行的唇色（例如楓紅、奶茶棕等等），也會利用程式篩選出包含「色」的字，並參考這些字的出處文章，再決定是否將這個顏色納入關鍵字中（例如文章提及「白色」並不是指白色唇膏，而是唇膏盒的包裝）\n3. 整理關鍵字 在這些關鍵字中，許多字的字面不同，卻代表同一意思。比如「潤感」、「潤光」都在表達「潤澤」，於是我就建立excel表，將「潤感」、「潤光」儲存在「關鍵字」coulmn，「潤澤」儲存在「對應」coulmn。最後整理完的關鍵字和顏色無關的可見於字典.xlsx，和顏色有關的則存在顏色.xlsx。\n4. 根據不同留言帳號，統計出他們各自使用關鍵字的次數 想要分析每位留言者的偏好其中一個方式就是找到他們喜歡的關鍵字。在這個環節，我統計每則留言出現的目標關鍵字，再根據留言帳號疊加起來，得到每位留言者在他所有的留言中使用目標關鍵字的總數。\n然而光看留言有會無法知道留言者在探討的議題，這時我們需要回去看留言所指的內文在寫什麼。因此我也將留言所指的內文納入考量，以得到更完整的留言者想法。我們的目標是利用統計關鍵字在留言中出現的次數得知留言者的偏好。而比起內文，留言本身更能代表留言者。所以在計算每則留言出現的目標關鍵字次數時，我的算法是以一比九的權重將內文出現的關鍵字次數與留言中出現的關鍵字次數相加起來。\n在上一個環節，我建立了字面不同的相似關鍵字對應到同個字詞的表格。在統計目標關鍵字出現次數的過程中，每當關鍵字出現我就會利用這個表格將關鍵字對應到所屬的字詞類別，最後再相加起來。例如「潤感」、「潤光」都在表達「潤澤」，所以在統計完「潤光」、「潤感」出現次數後要把這兩個關鍵字相加起來，得到「潤澤」出現的次數。如此一來便能較精準的算出目標關鍵字真正的聲量，避免目標關鍵字因表達方式不同而在文章及留言被忽略的問題。\n在得到所有目標關鍵字在每則留言出現的個數後，我依據留言者將他們疊加起來，最後算出每位留言者使用目標關鍵字的次數，並用這些數字代表留言者對不同主題偏好的程度。\n5. K-means分群 在上一步我們得到每個留言者對每個關鍵字的使用次數，詳細資料可見於excel檔\n而這些關鍵字使用次數代表偏好程度，因此我們可以用K-Means分群法將這些統計量做向量分群，得到需求、偏好各字不同的族群，掌握市場上不同面貌的消費者。\n為了知道分群數多少是最好的，我寫了for迴圈將資料做多次k-means分群，分群數介於1~15之間。最後將分群數對inertia (每個點到所屬群質心距離的和，用來衡量分群的誤差)，結果如下\n分的群數愈多，通常 inertia會越小，當群數等於資料點時必為零，其中的道理和隨機森林切得越細在訓練資料的誤差上越小相似。而要衡量分多少群最好，除了要讓inertia盡可能的小，還要讓分群數盡可能的少。所以一般來說大多會選擇 inertia 變化開始明顯變小附近的轉折點。最後我選擇n=6作為分群數。\n在做k-means分群時，我只採用與顏色無關的關鍵字作為feature，減少分群的複雜度。我們也能看看分群後的需求不同的消費者是不是比較偏好某些顏色。\n視覺化成果 我們共可得到六群顧客，而由於group 2 只有1人因此我不討論它。以下是去除group 2資料，用Tableau視覺化的成果：\n從表示各群人數的圓餅圖我們能看到，group0 人數440人為最大宗，group 5 人數84人為第二大群，group 1、3、4人數20人以下為小眾。\n我用Tree map表示每群顧客對不同關鍵字的偏好程度，在Tree map中板塊越大者關鍵字出現次數越多，代表該族群消費者對該關鍵字越偏好，需求也越高。\n從這五張Tree map我們能看到：\n不管是哪群顧客，霧面、秋冬幾乎都是出現最多的關鍵字。這可能是因為秋冬最流行霧面唇妝。\ngroup 0為最大族群（440人），因此我定義該群為大眾流行，也代表大多數女孩的需求。\nGroup 0 大眾流行的Tree map告訴我們：\n＊在唇妝上大家最偏好流行的霧面，潤澤次之 ＊功能上她們最注重護唇，也很注重滋潤度和保濕度，喜歡用羊脂膏 ＊她們普遍擔心唇紋問題，可能是秋冬較乾燥且她們常用相較其他妝感較顯唇紋的霧面唇膏的緣故 ＊風格上他們較偏好可愛、少女風格 ＊除了護唇也很注重顯白，顯氣色的腮紅和紅唇討論度也蠻高的 ＊喜歡薄擦唇膏 group1 有8人，他們特別偏好日系妝容（相較其他群該關鍵字使用次數多），也很注重唇膏的滋潤度、唇紋問題。她們在擦唇膏前會先打底，顏色上會選可以顯白的，此外除了流行的霧面唇膏她們也偏好潤澤唇膏。 group 3 有18人，特別偏好自然妝容，比起其他群的女孩更注重唇部保養和氣色，且對香奈兒的超炫耀系列討論度特別高，顏色上會挑顯白、深色的顏色，也很注重唇膏的顯色，推測是喜歡歐美風格的女孩 Group 4 有17人，他們非常注重顯白，也喜歡深色唇膏。風格偏好溫柔可愛的日常風，與其他群不同的是她們特別偏好暈染唇妝，上唇膏前會打底薄擦，也會注重搭配眼妝。另外，他們似乎也偏好限量商品。因為他們喜歡暈染唇妝，所以我叫他們韓系女孩。 Group 5有84人，除了流行的霧面唇妝，她們注重絲絨、粉霧那種較朦朧的感覺，顏色上喜歡暖系，注重唇膏的顯色和潤澤。 各族群對顏色的偏好 在依留言者對唇膏的需求進行分類後，我也統計每個族群提到有關唇膏顏色關鍵字的次數作為判斷他們對顏色偏好的依據。\n為了方便比較不同族群間對各顏色的偏好程度，我將每個族群提及顏色關鍵字的次數統計起來並除以他們的人數，計算族群中平均每個人提及顏色關鍵字的次數作為比較的標準。\n視覺化後的結果如下圖：\n首先我們先分析每個族群喜歡的顏色：\n除了Group5 外，其他族群幾乎都是最偏好紅色系唇膏。 大眾流行以紅色為最大宗，粉色系其次，裸色系第三。其他還有紅棕色系、橘色系、奶油系質感唇膏的需求存在。 Group 1日系美眉偏好紅色與紅棕色系的唇膏，同時他們對土色系唇膏偏好度大於其他族群，因此我判斷他們喜歡紅中帶棕、紅中帶咖啡色的唇膏。另外，他們也偏好具奶油質感的唇妝。 Group 3喜歡歐美風、香奈兒超炫耀的女孩以紅色唇膏為需求最大宗，此外，他們也是除了Group4韓系女孩外對紅色系需求最高的族群。值得注意的是，他們對裸色系唇膏的需求比起其他族群更高，符合歐美女性的偏好。而比起其他族群，他們對茶色、紅棕、土色唇膏需求較少 Group 4韓系女孩對紅色唇膏需求最大，而且也是各系女孩中對紅色唇膏需求最大的。由前段分析推測可能是他們比起其他族群更注重顯白的緣故。另外，他們也偏好粉色、橘色較清新的色系，和日系女孩一樣也偏好奶油系質感的唇膏，對裸色、紅棕咖啡、土色茶色需求相對少。 Group 5慕斯女孩與其他系女孩不同，對紅色唇膏需求反而沒那麼大。他們喜歡橘色系唇膏，粉色系次之。呼應前段分析顯示他們偏好暖色系的部分。推測他們可能喜歡活潑清新的風格。 若將各色系顏色細分，各族群偏好則如下圖\n顏色偏好（顏色）\n較值得注意的地方是Group1日系女孩在紅棕色系中偏好栗子色;Group5慕斯女孩則偏好楓色，此外他們對粉色系的偏好也以玫瑰色為主，橘色系則是橙色與栗橘。\n如何將專案結果運用於實務 除了用來釐清市場需求，了解顧客族群的樣貌，這個結果可以應用在推薦系統上。我們可以搜集消費者在留言平台上的留言，然後萃取他使用的關鍵字，進行統計後丟入預先訓練好的K-means模型預測出他可能是屬於哪類型的消費者，再做個人化推薦。比如我們預測消費者是歐美系女孩，我們便可多推薦超炫耀唇膏或紅色、裸色系的唇膏。\n此外，我們也可以應映分析出來的市場需求決定怎麼推廣或改良我們的產品。從視覺化報表中，我們可以看到大眾的需求圍繞著霧面唇膏、護唇保濕、避免唇紋、顯白等等，所以我們便可朝著滿足這些需求的方向改良、廣告我們的唇膏。另外，大眾普遍喜歡可愛風格，我們便可找形象類似的女明星來代言產品。\n改進方向 這個專案是以搜集留言者關鍵字為導向，而我們對關鍵字選擇和了解程度會很大程度的影響分析結果。因此可以事先諮詢相關專家，選擇能精準描述唇膏相關需求的關鍵字，並了解關鍵字不同的表達方式，可能會比自己上網找文章或直接從jieba切分的字詞了解有效率且精準。\n","permalink":"https://usausagichan.github.io/english_version/blog/cosmetic_english/","tags":["Python","Tableau","Scraping","Machine Learning"],"title":"PTT輿情分析-秋冬流行唇妝與顧客分群"},{"categories":["Project"],"contents":"If you are an investor in the Taiwan stock market, you are likely familiar with the Yuanta 0050 ETF(0050.TW). As one of the largest index funds in Taiwan, it tracks the Taiwan 50 Index, which can be simply considered as a number allocated proportionally according to the market value of top 50 companies by market capitalization in Taiwan. The list or proportion of the portfolio will be adjusted on a rolling basis based on changes in the market value of these companies. Therefore, we can grasp the approximate industry distribution and mainstream companies in Taiwan from the portfolio composition of 0050. Overall, 0050.TW can be regarded as S\u0026amp;P500 of Taiwan version with 50 companies only.\nIn this project, I first used Python to retrieve the historical portfolio composition of the Yuanta 0050 ETF from the Taiwan Stock Exchange\u0026rsquo;s Market Observation Post System (https://mops.twse.com.tw/mops/web/t78sb04_q2). Then, I used Tableau to visualize and analyze the changes in Taiwan\u0026rsquo;s industry distribution and mainstream companies over time.\nRegarding the part of web scraping, I have uploaded the code to my Github: https://github.com/usausagichan/data-mining-for-0050.Tw-. Interested friends can refer to it for learning purposes.\nIn the following article, I will present the visualized results of this data using Tableau (https://public.tableau.com/app/profile/ctchen/viz/IndustryAnalysis_16698030444010/Dashboard1 for the complete dashboard), and provide some analysis.\nData from Taiwan Stock Exchange\u0026rsquo;s Market Observation Post System\nThe captured data is shown in the following table, which includes the stock name(股票名稱), industry category(產業類別), and shareholding ratio(持股比率). The column \u0026ldquo;持股比率\u0026rdquo; represents the proportion of a single company\u0026rsquo;s stock holdings, and \u0026ldquo;持股比率.1\u0026rdquo; represents the proportion of the industry category.\nThe following is the visualization of the shareholding ratios of each industry and company in the third quarter of this year for the E Fund 50 (0050.TW):\nExtracting mainstream Taiwanese companies and industries from the word cloud and treemap:\nFigure 1\nThe sizes and shades of the Treemap and word cloud represent the percentage of each industry and company in the holdings of the Yuanta 0050 ETF, which roughly reflects the market value of each major company and industry in the Taiwan stock market. From Figure 1 we could see that in 2022, the semiconductor industry is the largest sector in Taiwan, followed by the financial and insurance industry, which accounts for about 53.79% and 15.86% of the Yuanta 0050 holdings, respectively, with a difference of more than 3.39 times. Other tech industries such as electronics, telecommunications, electronic components, computers and peripherals, optoelectronics, as well as traditional industries such as steel, food, and cement are also included. TSMC is the largest company in terms of market value, and even Hon Hai, the second-largest, is far behind.\nFigure 2\nFocusing on the semiconductor industry (left side of Figure 2), we can also see that TSMC overwhelmingly dominates, with a share almost 12.37 times that of the second-place MediaTek. On the other hand, the second-ranked financial and insurance industry is evenly distributed (right side of Figure 2), with the top five companies accounting for about 1.64% to 1.47% of the portfolio. In fact, the ranking of these companies\u0026rsquo; shareholdings in the 0050.TW may vary each quarter, and their shareholdings do not exactly reflect their market capitalization. This is because the calculation of the TAIEX 0050 also takes into account the public float of individual stocks, and besides, the market capitalization of these companies is not significantly different. As the result, public float may seriously affect their ranking in the 0050.TW portfolio.\nIn summary, Taiwan\u0026rsquo;s industries are heavily focused on the semiconductor and electronics-related technology industries, as well as the finance and insurance industry. However, the employment requirement for these industries is higher than other industries, particularly in the case of the semiconductor industry, which is dominated by TSMC. This make the number of people employed by companies that can access these large resources limited. According to a 2021 report by the Commercial Times[1], employment in the Taiwan IC semiconductor industry is only about 7%, and TSMC employees accounted for just over 60,000 people in the previous year. Furthermore, seven to eight percent of people work in traditional industries. In other words, the population working for these large companies we see in Taiwan is a minority, and those who benefit from high salaries provided by these large market cap companies are also a minority. This may lead to a wealth gap, and the situation and salaries received by most people may not be reflected in Taiwan\u0026rsquo;s economic indicator, the Taiex 0050. In addition, in an economy that heavily relies on industries with high electricity demands such as TSMC and the electronics industry, a shortage of electricity in the future could cause serious economic problems.\nThe relationship between stock price of TSMC and 0050.TW\nFigure 3\nIn essence, the existence of an investment portfolio is to hedge against risks, and this applies to ETF as well. However, is it really okay for the 0050.TW to have nearly half of its funds invested in TSMC given the situation where TSMC is dominating the market? Looking at the historical closing price data for 0050.TW and TSMC (2330.TW) this year (Figure 3), we can see that their trends are almost identical, indicating that TSMC\u0026rsquo;s ups and downs determine the performance of 0050.TW. Judging from the proportion of the investment portfolio, even if Hon Hai, the second-largest market capitalization holding with a 5.42% share, rises by 8.33%, its impact on 0050.TW\u0026rsquo;s return rate is only equal to that of TSMC\u0026rsquo;s 1% increase, let alone other stocks with smaller proportions.\nFigure 4\nIn fact, among all fifty stocks, only four have a shareholding ratio of more than 2% - TSMC, Hon Hai, MediaTek, and Delta Electronics (Figure 4 left); and only twenty-two stocks have a shareholding ratio of more than 1% (Figure 4 right). That is to say, about half of the stocks in the 0050.TW investment portfolio have a proportion of less than 1%, and their influence can be described as negligible compared to TSMC\u0026rsquo;s.\nFigure 5 0050.TW in 2008 and 2019\nTherefore, from the perspective of diversified investment, buying the 0050.TW may not be a good choice. Since its listing in 2003, Taiwan Semiconductor Manufacturing Company (TSMC) has almost always been the largest holding in the portfolio, with a stable percentage range of 10-20%, until the fourth quarter of 2012 when its percentage started to exceed 20%. In the following years, TSMC\u0026rsquo;s percentage continued to increase, reaching about 40% by the end of Q4 2019, when its stock price hit a new high of 331 Taiwan dollars, up from 219.5 Taiwan dollars at the beginning of the year. If we look at the sector distribution in 2008 (Figure 5, left) and 2019 (Figure 5, right), we can see that TSMC\u0026rsquo;s percentage in the Taiwan index has increased significantly over the past decade, and has led to a further concentration of Taiwan\u0026rsquo;s industry in the semiconductor sector.\nIn 2020, TSMC announced the mass production of its 5-nanometer process, which led to a significant influx of funds. In the third quarter of that year, the holding of TSMC by the ETF reached 48%, an increase of about 8% from the previous quarter, reflecting a significant increase in market capitalization in a short period of time. Looking at the stock price alone, from the stock market crash due to the COVID-19 panic in March 2020 to January of the following year, TSMC\u0026rsquo;s stock price rose from NT$248 to NT$673, a whopping 2.71 times increase. However, since the beginning of this year (2022), TSMC\u0026rsquo;s stock price has been declining along with the rising US interest rates. To make matters worse, after Pelosi\u0026rsquo;s visit to Taiwan in August of this year, geopolitical risks have increased, and in the midst of foreign investors\u0026rsquo; rush to withdraw their investments, the stock price continued to fall until recently when it showed signs of stabilizing and rebounding.\nFigure 6 0050.TW and TSMC\u0026rsquo;s stock price in 2019-2022 and 2008-2019\n​\nWe can compare the historical prices of TSMC and the 0050.TW from early 2008 to the end of 2018 (left chart in Figure 6) and from early 2019 to the end of 2020 (right chart in Figure 6). It is clear that the price trends of TSMC and the 0050.TW Taiwan Top 50 ETF in the three years from 2019 to 2022 are closer to each other than in the previous ten years before 2018. On the one hand, this is because TSMC\u0026rsquo;s market value began to increase significantly in 2019, thereby increasing its shareholding proportion and making its influence on the 0050.TW Taiwan Top 50 ETF stronger. On the other hand, as TSMC is Taiwan\u0026rsquo;s largest stock and has almost become an industry leader in the past three years, it is bound to attract investors\u0026rsquo; attention from all over the world. With large amounts of funds flowing in and out of the stock, the volatility of TSMC is inevitable, which in turn affects the 0050.TW. Under these two factors combined, the price trend of the 0050.TW is largely determined by TSMC, hence the joke that buying 0050.TW is equivalent to buying TSMC.\nConclusion\nVisualizing the holdings of the Yuanta Taiwan 50 ETF, we can first see that Taiwan\u0026rsquo;s industry funds are heavily concentrated in the semiconductor, financial, and electronics sectors, with TSMC dominating the semiconductor industry. From these conclusions, we can observe the potential wealth gap in Taiwan\u0026rsquo;s job market and the high dependence of Taiwan\u0026rsquo;s economy on electricity. Moreover, with nearly half of the Yuanta Taiwan 50 ETF\u0026rsquo;s holdings concentrated in TSMC, this not only reflects Taiwan\u0026rsquo;s economy\u0026rsquo;s high dependence on TSMC but also limits the fund\u0026rsquo;s hedging ability. If TSMC were to experience a massive decline in a short period, and the fund were unable to adjust its holdings quickly enough to minimize losses, it would undoubtedly be a disaster for those holding the fund. Therefore, those holding this stock should pay close attention to TSMC\u0026rsquo;s profitability and company condition to truly avoid risk, which may not meet the expectations of many who expect \u0026ldquo;brain-free investing\u0026rdquo; from ETFs.\nReference\n[1] https://ctee.com.tw/bookstore/magazine/426808.html\n","permalink":"https://usausagichan.github.io/english_version/blog/0050_english/","tags":["Python","Tableau","Scraping"],"title":"See the industry structure from stock 0050.TW"},{"categories":["Project"],"contents":"一間房子到底要賣多少錢才符合行情？我們最直接想到的判斷要素應該會是這個房子的地段和坪數。實際上不只這兩點，還有許多因素會影響房價。我們就用kaggle上的波士頓房價資料集來看這件事吧\n在資料前處理的部分我會\n將資料視覺化，並對資料做一些轉換和剔除不重要或互相有強烈關聯的feature 了解各feature代表的意義，適當的填入缺失值和做one-hot coding 用Lasso做feature selection 接著進入training 的環節，我使用的model是XGBoost，以5-fold cross validation選出模型合適參數，最後用train出來的模型做房價預測\n首先匯入套件和training, testing data set\n#load module, data import pandas as pd import numpy as np import sklearn as sci import matplotlib.pyplot as plt import seaborn as sns import scipy.stats as stats import xgboost as xgb from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LassoCV from sklearn.feature_selection import SelectFromModel train = pd.read_csv(\u0026#34;train.csv\u0026#34;) test = pd.read_csv(\u0026#34;test.csv\u0026#34;) Data Cleaning： 我將training和testing data的feature先合併以利接下來做統一處理，除了方便外也可以避免分開做歸一化會有尺度不相同的問題\ndtrain=train.drop([\u0026#39;SalePrice\u0026#39;],axis=1) database=pd.concat([dtrain,test]) 首先我大致將資料分為’object_type’(qualitative)和’numerical_type’(quantative)\n而由官網中data_description檔案對各feature的細節描述，\nhttps://drive.google.com/file/d/17sT4kM4qbzHlQ9zO-V1ZK81Gdlm8YVQ6/view?usp=sharing\n可以得知feature ‘MSSubClass’中的整數事實上是代表房屋交易的不同類別，並且數字大小和他的實際意義完全沒有關係，所以要把他的資料類別改成字串\n#datacleaning realobj=[\u0026#39;MSSubClass\u0026#39;] database[realobj]=database[realobj].astype(str) quant = [f for f in database.columns if database.dtypes[f] != \u0026#39;object\u0026#39;] quali = [f for f in database.columns if database.dtypes[f] == \u0026#39;object\u0026#39;] 判定其中存在缺失的column為\nIndex([**\u0026#39;TotalBsmtSF\u0026#39;, \u0026#39;GarageArea\u0026#39;, \u0026#39;GarageCars\u0026#39;**, \u0026#39;KitchenQual\u0026#39;, \u0026#39;Electrical\u0026#39;, **\u0026#39;BsmtUnfSF\u0026#39;, \u0026#39;BsmtFinSF2\u0026#39;, \u0026#39;BsmtFinSF1\u0026#39;**, \u0026#39;SaleType\u0026#39;, \u0026#39;Exterior1st\u0026#39;, \u0026#39;Exterior2nd\u0026#39;, \u0026#39;Functional\u0026#39;, \u0026#39;Utilities\u0026#39;, **\u0026#39;BsmtHalfBath\u0026#39;, \u0026#39;BsmtFullBath\u0026#39;**, \u0026#39;MSZoning\u0026#39;, **\u0026#39;MasVnrArea\u0026#39;**, \u0026#39;**MasVnrType\u0026#39;**, \u0026#39;**BsmtFinType1**\u0026#39;, \u0026#39;**BsmtFinType2**\u0026#39;, \u0026#39;**BsmtQual**\u0026#39;, \u0026#39;**BsmtCond**\u0026#39;, \u0026#39;**BsmtExposure**\u0026#39;, \u0026#39;**GarageType**\u0026#39;, \u0026#39;**GarageCond**\u0026#39;, \u0026#39;**GarageQual**\u0026#39;, \u0026#39;**GarageYrBlt**\u0026#39;, \u0026#39;**GarageFinish**\u0026#39;, \u0026#39;LotFrontage\u0026#39;, \u0026#39;**FireplaceQu**\u0026#39;, **\u0026#39;Fence\u0026#39;, \u0026#39;Alley**\u0026#39;, **\u0026#39;MiscFeature\u0026#39;**, \u0026#39;**PoolQC**\u0026#39;] 某些feature中的’NA’代表本身無該項feature，但pandas會把他判定成資料有缺失，對於這類feature ‘object_type’要置換成type-’None’，’numerical_type’要補零，比如’Pool QC’ (Pool Quality)的’NA’值代表沒有泳池，把NA改成’None’即可，其餘的缺失值’object_type’用該column的眾數補上，’numerical_type’則用該column平均值補上\nnon=[\u0026#39;PoolQC\u0026#39;, \u0026#39;MiscFeature\u0026#39;, \u0026#39;Alley\u0026#39;, \u0026#39;Fence\u0026#39;, \u0026#39;FireplaceQu\u0026#39;, \u0026#39;GarageCond\u0026#39;, \u0026#39;GarageQual\u0026#39;, \u0026#39;GarageFinish\u0026#39;, \u0026#39;GarageType\u0026#39;, \u0026#39;BsmtCond\u0026#39;, \u0026#39;BsmtExposure\u0026#39;, \u0026#39;BsmtQual\u0026#39;, \u0026#39;BsmtFinType1\u0026#39;, \u0026#39;BsmtFinType2\u0026#39;,\u0026#39;MasVnrType\u0026#39;] zerof=[\u0026#39;TotalBsmtSF\u0026#39;, \u0026#39;GarageArea\u0026#39;, \u0026#39;GarageCars\u0026#39;,\u0026#39;BsmtUnfSF\u0026#39;, \u0026#39;BsmtFinSF2\u0026#39;, \u0026#39;BsmtFinSF1\u0026#39;,\u0026#39;BsmtHalfBath\u0026#39;, \u0026#39;BsmtFullBath\u0026#39;,\u0026#39;MasVnrArea\u0026#39;] needfill=[\u0026#39;KitchenQual\u0026#39;,\u0026#39;Electrical\u0026#39;,\u0026#39;SaleType\u0026#39;,\u0026#39;Exterior1st\u0026#39;,\u0026#39;Exterior2nd\u0026#39;,\u0026#39;Functional\u0026#39;, \u0026#39;Utilities\u0026#39;,\u0026#39;MSZoning\u0026#39;]#front: numerical_type_feature backw: objective_type_feature front=database[quant] front[zerof]=front[zerof].fillna(0) front[\u0026#39;LotFrontage\u0026#39;]=front[\u0026#39;LotFrontage\u0026#39;].fillna(front[\u0026#39;LotFrontage\u0026#39;].mean())backw=database[quali] for x in needfill: backw[x]=backw[x].fillna(backw[x].mode()[0]) backw[non]=backw[non].fillna(\u0026#39;None\u0026#39;) 視覺化各統計資料： ‘numerical_type_feature’數值分佈圖 c= [f for f in database.columns if database.dtypes[f] != \u0026#39;object\u0026#39;] c.remove(\u0026#39;Id\u0026#39;) f = pd.melt(front, value_vars=c) g = sns.FacetGrid(f, col=\u0026#34;variable\u0026#34;, col_wrap=4 , sharex=False, sharey=False) g = g.map(sns.distplot, \u0026#34;value\u0026#34;) 在這些分佈圖中，我們可以看出’LotFrontage’, ‘LotArea’ ,’1stFlrSF’ ,’GrLivArea’的分佈較接近log normal distribution，因此可以先將這些column的數值取log，使資料的分佈接近常態分佈，接著再對所有numerical_type_feature做normalization\n另外’BsmtFinSF2\u0026rsquo;, ‘LowQualFinSF’, ‘BsmtHalfBath’, ‘KitchenAbvGr’, ‘EnclosedPorch’, ‘3SsnPorch’, ‘ScreenPorch’, ‘PoolArea’, ‘MiscVal’值非常集中且單一，無法從中得到資訊，可納入刪除名單\nlogtran=[\u0026#39;LotFrontage\u0026#39;,\u0026#39;LotArea\u0026#39;,\u0026#39;1stFlrSF\u0026#39;,\u0026#39;GrLivArea\u0026#39;] front[logtran]=np.log(front[logtran]) front=(front-front.mean())/(front.std()) 觀察Sale Price的數據，會發現它的分布是比較偏斜的，這樣的資料不利於Training，因為預測結果大量集中在某個區間範圍除了可能會讓機器忽略其他較離散的數值，也會讓機器較難分辨出不同feature對應到的不同預測值。這時我們通常會取log讓預測值的分佈散開。實際上、當我們將Sale Price的分佈對做擬合會發現它的分布其實很接近log normal distribution (即取log 後成常態分佈)\ny = train[\u0026#39;SalePrice\u0026#39;] plt.figure(2); plt.title(\u0026#39;Normal\u0026#39;) sns.distplot(y, kde=False, fit=stats.norm) plt.figure(3); plt.title(\u0026#39;Log Normal\u0026#39;) sns.distplot(y, kde=False, fit=stats.lognorm)trainresult=np.log(data[\u0026#39;SalePrice\u0026#39;]) 繪製numerical_feature和Sale_Price的熱力圖觀察feature間的相關性大小：\nplt.figure(figsize=(16, 16)) front=front.drop(\u0026#39;Id\u0026#39;,axis=1) front[\u0026#39;SalePrice\u0026#39;]=np.log(train[\u0026#39;SalePrice\u0026#39;]) corr = front.corr() sns.heatmap(corr,cmap=\u0026#34;viridis\u0026#34;) 由上圖可看出TotalBsmt和1stFLSF，garage_area和garage_car有強烈關聯性，依據常理推斷是因為地下室總面積和第一層樓的面積大多數情況應是相同的，而garage面積愈大garage_car也會愈多\n另外’OverallQual’ , ‘GrLivArea’和’Sale_Price’ 相關性比較大，可推測’OverallQual’ , ‘GrLivArea’會是training過程中權重較大的重要feature\n至於看起來不太重要的feature（整列顏色都很黯淡）有：\n‘BsmtFinSF2’, ‘LowQualFinSF’, ‘BsmtHalfBath’, ‘KitchenAbvGr’, ‘EnclosedPorch’, ‘3SsnPorch’, ‘ScreenPorch’, ‘PoolArea’, ‘MiscVal等等\n而以上提及的這些不重要的feature正是前面提到『分佈非常集中且單一』的feature\n另外也可看出’MoSold’ ‘YrSold’不太重要，可見銷售時間對房價沒有太大影響\n接著對’object_type’ feature做one-hot coding，並將’object_type’和’numerical_type’ feature\n重新組回單一dataframe，之後剔除根據data_visualization的結果判定不重要的feature\n除此之外，某些’object_type’ feature中標示為None的對象是重複的，比如同筆交易資料顯示房子無Garage，則該筆資料中’GarageCond’, ‘GarageQual’, ‘GarageFinish’, ‘GarageType’通通都會是’None’，而經過one-hot coding處理後會產生’GarageCond_None’, ‘GarageQual_None’, ‘GarageFinish_None’ , ‘GarageType_None’等feature，此時這筆資料在這些feature中值通通都是1因；若該筆交易資料的房子有Garage則為零，即這些feature的值完全相同。因此我們只要保留一個feature即可\nbackw=pd.get_dummies(backw) databass=pd.concat([front,backw],axis=1) delete_feature=[\u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;,\u0026#39;Id\u0026#39;,\u0026#39;BsmtHalfBath\u0026#39;, \u0026#39;KitchenAbvGr\u0026#39;, \u0026#39;EnclosedPorch\u0026#39;, \u0026#39;3SsnPorch\u0026#39;, \u0026#39;ScreenPorch\u0026#39;, \u0026#39;PoolArea\u0026#39;, \u0026#39;MiscVal\u0026#39;,\u0026#39;1stFlrSF\u0026#39;,\u0026#39;GarageArea\u0026#39;,\u0026#39;LowQualFinSF\u0026#39;, \u0026#39;BsmtFinSF2\u0026#39;,\u0026#39;GarageYrBlt\u0026#39;] repeatnon=[ \u0026#39;GarageCond_None\u0026#39;,\u0026#39;GarageQual_None\u0026#39;, \u0026#39;GarageFinish_None\u0026#39;, \u0026#39;GarageType_None\u0026#39;, \u0026#39;BsmtCond_None\u0026#39;,\u0026#39;BsmtExposure_None\u0026#39;, \u0026#39;BsmtQual_None\u0026#39;, \u0026#39;BsmtFinType1_None\u0026#39;, \u0026#39;BsmtFinType2_None\u0026#39;,\u0026#39;MasVnrType_None\u0026#39;] databass=databass.drop(delete_feature,axis=1) databass=databass.drop(repeatnon,axis=1) ‘numerical_type_feature’對Sale-Price的scatter plot def scatter(x,y,**kwargs): sns.scatterplot(x,y) c= [f for f in database.columns if database.dtypes[f] != \u0026#39;object\u0026#39;] c.remove(\u0026#39;Id\u0026#39;) front[\u0026#39;SalePrice\u0026#39;]=train[\u0026#39;SalePrice\u0026#39;] f = pd.melt(front, id_vars=[\u0026#39;SalePrice\u0026#39;],value_vars=c) g = sns.FacetGrid(f, col=\u0026#34;variable\u0026#34;, col_wrap=4 , sharex=False, sharey=False) g = g.map(scatter, \u0026#34;value\u0026#34;,\u0026#39;SalePrice\u0026#39;) 從上面這些圖來看，觀察前文分析出來可能較重要的feature: ‘OverallQual’ , ‘GrLivArea’，可發現在’OverallQual’最大的情況下有兩個點SalePrice偏低， 同樣的情況也可見於’GrLivArea’，這兩個點index為523及1298，明顯不符常理和其他點的趨勢，為異常值應去除\n用Lasso選取重要feature: Lasso是一種透過加入一次懲罰係數做regularization的線性模型，而在經過training後不重要的feature係數會變成零，是常被用來做feature selection的方式。並且，在training同時可以透過GridSearchCV的方式去找最合適的懲罰項係數，讓train出來最準確的模型判斷feature的重要性，留下權重大於0的項\n#feature_selection by Lasso() pipeline = Pipeline([(\u0026#39;scaler\u0026#39;,StandardScaler()),(\u0026#39;model\u0026#39;,Lasso())]) search = GridSearchCV(pipeline,{\u0026#39;model__alpha\u0026#39;:np.arange(1e-3,1e-2,1e-4)}, cv = 5, scoring=\u0026#34;neg_mean_squared_error\u0026#34;,verbose=5) search.fit(databass.iloc[:1458], trainresult) print(search.best_params_) coefficients = search.best_estimator_.named_steps[\u0026#39;model\u0026#39;].coef_ importance = np.abs(coefficients) feature=databass.columns evaluate=pd.DataFrame({\u0026#39;feature\u0026#39;:feature,\u0026#39;importance\u0026#39;:importance}) drop=evaluate[evaluate[\u0026#39;importance\u0026#39;]==0] databass=databass.drop(drop[\u0026#39;feature\u0026#39;],axis=1) Training Process: 在training process 中，我一共用了四個線性模型（Ridge, Lasso, Elastic Net, SVM Regressor）和兩個Boosting模型（Gradient Boosting, XGB）以及以前述六個模型當作第一階段的regressor，XGB演算法做第二階段的meta regressor做stacking，然後將stacking的模型和其他模型一起做blending\n重新將training和testing data分開，定義cv_rmse_function為接下來評估各model使用的方法-10-fold cross validation做準備\ntrainfeature=databass[:1458] #2 outlier removed testfeature=databass[1458:] #define functions in training process kfolds = KFold(n_splits=10, shuffle=True, random_state=42)def cv_rmse(model, X, y): rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\u0026#34;neg_mean_squared_error\u0026#34;, cv=kfolds)) return (rmse) 設定六種模型的參數值，在linear model上使用RobustScaler()減少離群值對training process的影響及stacking的方式（first stage regressor=ridge, lasso, elasticnet,svr, gbr, xgbr, meta regressor= XGB）\nalphas_alt = np.arange(14,16,0.1) alphas2 = [0.003, 0.004, 0.005] e_alphas = np.arange(1e-4,1e-3,1e-4) e_l1ratio = np.arange(0.8,1,0.25)ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds)) lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds)) elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio)) svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,)) xgbr= xgb.XGBRegressor(learning_rate= 0.01, max_depth= 3, n_estimators= 2500,objective=\u0026#39;reg:linear\u0026#39;) gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features=\u0026#39;sqrt\u0026#39;, min_samples_leaf=15, min_samples_split=10, loss=\u0026#39;huber\u0026#39;, random_state =42) stackr = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,svr, gbr, xgbr), meta_regressor=xgbr,use_features_in_secondary=True) 分別得到六個模型在training data set上的10-fold cross validation error，並記錄下來，當作後面做blending比例的參考\ns=[] score = cv_rmse(ridge,trainfeature, trainresult) s.append(1/score.mean()) print(\u0026#34;Ridge: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std()) ) score = cv_rmse(lasso,trainfeature, trainresult) s.append(1/score.mean()) print(\u0026#34;LASSO: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) score = cv_rmse(elasticnet,trainfeature, trainresult) print(\u0026#34;elastic net: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std()) ) s.append(1/score.mean()) score = cv_rmse(svr,trainfeature, trainresult) print(\u0026#34;SVR: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) s.append(1/score.mean()) score = cv_rmse(xgbr,trainfeature, trainresult) print(\u0026#34;xgboost: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) s.append(1/score.mean()) score = cv_rmse(gbr,trainfeature, trainresult) print(\u0026#34;gboost: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) s.append(1/score.mean()) 讓七個模型各自做training\nlasso_mod= lasso.fit(trainfeature, trainresult) ridge_mod= ridge.fit(trainfeature, trainresult) elasticnet_mod= elasticnet.fit(trainfeature, trainresult) svr_mod = svr.fit(trainfeature, trainresult) xgb_mod= xgbr.fit(trainfeature, trainresult) gbr_mod= gbr.fit(trainfeature, trainresult) stack_mod= stackr.fit(np.array(trainfeature), np.array(trainresult)) 以六個模型（Ridge, Lasso, Elastic Net, SVM Regressor）依據各自的10-fold cross validation error的倒數做加總（註：為使error愈大的模型權重排名愈小，error愈小權重愈大），最後再與Stacking的模型做平均得到最終predict的結果\n#blending s=(s/np.sum(s)) def blend_predict(X): return np.exp(0.5*((s[0] * ridge_mod.predict(X))+ (s[1] * lasso_mod.predict(X)) + (s[2] * elasticnet_mod.predict(X)) + (s[3] * svr_mod.predict(X)) + (s[4] * xgb_mod.predict(X))+(s[5] * gbr_mod.predict(X)))+0.5*stack_mod.predict(np.array(X))) testresult=blend_predict(testfeature) 最後predict結果的public score大約是0.1243（score的計算方式為將預測值和實際值取log後計算mean square error，這樣取log後算error的好處是error值不會受房價本身大小的影響，因此可以用來衡量不同數量級房價預測的準確性）\n","permalink":"https://usausagichan.github.io/english_version/blog/houseprice_english/","tags":["Python","Machine Learning"],"title":"kaggle-波士頓房價預測"}]